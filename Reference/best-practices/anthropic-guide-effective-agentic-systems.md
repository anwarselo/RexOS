\# Anthropic Guide: Effective Agentic Systems Principles

\*Summary based on Anthropic's "Building Effective Agents" guide and related materials provided.\*

\#\# Core Philosophy

\* \*\*Simplicity First:\*\* Start with the simplest possible solution (e.g., single augmented LLM call). Only increase complexity (workflows, agents) if it demonstrably improves outcomes. Avoid complexity for complexity's sake.  
\* \*\*Frameworks Caution:\*\* Frameworks (LangGraph, Bedrock Agents, Rivet, Vellum etc.) can help get started but can add abstraction that hinders debugging. Understand the underlying code. Direct API calls are often sufficient initially.

\#\# Workflows vs. Agents

\* \*\*Workflows:\*\* Systems where LLMs and tools are orchestrated through \*\*predefined code paths\*\* (like most current n8n setups). Good for predictability and consistency in well-defined tasks.  
\* \*\*Agents:\*\* Systems where LLMs \*\*dynamically direct their own processes and tool usage\*\*, maintaining control over task accomplishment. Better for flexibility, open-ended problems, and scaling tasks in trusted environments, but potentially higher cost and risk of compounding errors. Requires robust tool design (ACI) and environmental feedback loops.  
    \* \*RexOS Context:\* Currently, RexOS primarily implements \*\*workflows\*\*. The multi-agent structure uses specialized workflows triggered by routing logic or specific inputs. True dynamic agency might be a future evolution.

\#\# Building Block: Augmented LLM

\* The foundation is an LLM enhanced with capabilities like:  
    \* \*\*Retrieval:\*\* Accessing external knowledge (like Pinecone/Baserow in RexOS).  
    \* \*\*Tools:\*\* Interacting with external APIs/services (like Baserow nodes, Google Calendar nodes, HTTP Request nodes in RexOS).  
    \* \*\*Memory:\*\* Retaining context across interactions (can be implemented via databases like Baserow/Supabase PGVector, or dedicated memory nodes in n8n).  
\* Focus on tailoring these capabilities and providing clear interfaces (good tool descriptions/prompting).

\#\# Effective Workflow Patterns

1\.  \*\*Prompt Chaining:\*\*  
    \* \*\*Concept:\*\* Decompose a task into sequential steps; output of one LLM call feeds the next. Can include programmatic checks ("Gates").  
    \* \*\*Use Case:\*\* Tasks easily broken into fixed subtasks; trading latency for higher accuracy on complex tasks.  
    \* \*RexOS Example:\* Could be used for complex report generation (Outline \-\> Draft Sections \-\> Combine \-\> Format).  
2\.  \*\*Routing:\*\*  
    \* \*\*Concept:\*\* Classify input and direct it to specialized downstream prompts, models, or tools.  
    \* \*\*Use Case:\*\* Handling distinct categories of input separately; optimizing cost/speed by using different models for different tasks.  
    \* \*RexOS Example:\* The AI Information Extractor \+ Switch node is a prime example of routing (Task vs. Remember vs. Log).  
3\.  \*\*Parallelization:\*\*  
    \* \*\*Concept:\*\* Run LLM calls simultaneously and aggregate results.  
        \* \*Sectioning:\* Break task into independent subtasks run in parallel (e.g., different agents checking different systems).  
        \* \*Voting:\* Run the same task multiple times/with different models for diverse outputs or higher confidence.  
    \* \*\*Use Case:\*\* Speeding up independent subtasks; getting multiple perspectives; implementing guardrails alongside main processing.  
    \* \*RexOS Example:\* Could potentially run morning checks by different agents (Mark for wellness, Michael for finance) in parallel before Alex synthesizes the summary.  
4\.  \*\*Orchestrator-Workers:\*\*  
    \* \*\*Concept:\*\* Central LLM dynamically breaks down complex tasks, delegates to specialized worker LLMs, and synthesizes results. Subtasks are not predefined.  
    \* \*\*Use Case:\*\* Complex tasks where needed subtasks are unpredictable (e.g., complex coding, multi-source research).  
    \* \*RexOS Example:\* A future Alex could act as orchestrator for a request like "Plan my trip to Italy," delegating flight research, accommodation booking, and activity planning to specialized workers/tools.  
5\.  \*\*Evaluator-Optimizer:\*\*  
    \* \*\*Concept:\*\* One LLM generates a response; another LLM evaluates it based on criteria and provides feedback in a loop until criteria are met.  
    \* \*\*Use Case:\*\* Tasks with clear evaluation criteria where iterative refinement adds value (e.g., writing/translation quality checks, complex search refinement).  
    \* \*RexOS Example:\* Could be used to refine drafted emails or reports generated by an agent before sending.

\#\# Agent-Computer Interface (ACI) / Tool Engineering

\* Crucial for both workflows with tools and autonomous agents.  
\* Treat tool definitions (name, description, parameters) like API documentation for the LLM.  
\* \*\*Make usage obvious:\*\* Use clear names, descriptions, examples, edge cases, boundaries.  
\* \*\*Keep format natural:\*\* Use formats close to what the LLM has seen online (e.g., JSON might require careful prompting for escaping vs. Markdown). Avoid unnecessary formatting overhead for the LLM.  
\* \*\*Test tool usage:\*\* See where the LLM makes mistakes and iterate on the tool definition/prompt.  
\* \*\*Poka-yoke (Mistake-Proof):\*\* Change arguments/structure to make errors less likely (e.g., require absolute paths instead of relative paths).

By keeping these principles and patterns in mind, we can make informed decisions as we continue building and refining RexOS.

